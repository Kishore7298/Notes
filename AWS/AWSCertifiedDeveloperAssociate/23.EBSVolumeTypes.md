### âš™ï¸ EBS Volume Types Overview

EBS (Elastic Block Store) offers **six types of volumes**, grouped into **SSD** (Solid State Drives) and **HDD** (Hard Disk Drives) categories.

```mermaid
mindmap
  root((EBS Volume Types))
    SSD Volumes
      gp2/gp3 (General Purpose)
      io1/io2 Block Express (Provisioned IOPS)
    HDD Volumes
      st1 (Throughput Optimized)
      sc1 (Cold)
```

---

### ðŸ§© SSD Volumes â€” High Performance

#### 1. **gp2 / gp3 â€” General Purpose SSD**

* **Goal:** Balance between cost and performance.
* **Use cases:** Boot volumes, virtual desktops, dev/test environments, general workloads.

| Attribute      | **gp2**                                  | **gp3**                                        |
| -------------- | ---------------------------------------- | ---------------------------------------------- |
| Volume Size    | 1 GB â€“ 16 TB                             | 1 GB â€“ 16 TB                                   |
| Baseline IOPS  | Size-dependent (3 IOPS/GB, up to 16,000) | 3,000 IOPS baseline                            |
| Max IOPS       | 16,000                                   | 16,000                                         |
| Max Throughput | ~250 MB/s                                | Up to 1,000 MB/s                               |
| Relationship   | IOPS linked to size                      | IOPS and throughput configurable independently |
| Use Case       | Older gen, predictable scaling           | New gen, cost-effective and flexible           |

**Key takeaway:**

* **gp3** decouples performance from size (you can set IOPS and throughput independently).
* **gp2** performance grows with size (3 IOPS per GB).

```mermaid
graph LR
    A[gp2 Volume Size â†‘] --> B[IOPS â†‘ linked]
    C[gp3 Volume Size] --> D[IOPS set independently]
    C --> E[Throughput set independently]
```

---

#### 2. **io1 / io2 Block Express â€” Provisioned IOPS SSD**

* **Goal:** For **mission-critical**, **low-latency**, **high-throughput** workloads (e.g., databases).
* **Performance** and **IOPS** are **user-provisioned** (not automatically scaled).

| Attribute            | **io1**                              | **io2 Block Express**                       |
| -------------------- | ------------------------------------ | ------------------------------------------- |
| Volume Size          | 4 GB â€“ 16 TB                         | Up to 64 TB                                 |
| Max IOPS             | 64,000 (Nitro EC2) / 32,000 (others) | 256,000                                     |
| Latency              | <1 ms                                | Sub-ms                                      |
| IOPS/GB Ratio        | Up to 50:1                           | Up to 1,000:1                               |
| Multi-Attach Support | âœ… Yes                                | âœ… Yes                                       |
| Durability           | 99.9%                                | 99.999%                                     |
| Use Case             | High I/O database                    | Enterprise-grade, ultra-low latency systems |

**Key takeaway:**

* For **databases** or **IO-intensive apps**, choose **io1/io2**.
* If you need **>32,000 IOPS**, you must use **EC2 Nitro** + **io1/io2**.

```mermaid
flowchart TD
    A[Provisioned IOPS SSD] --> B[io1 - up to 64K IOPS]
    A --> C[io2 Block Express - up to 256K IOPS]
    B --> D[Mission-Critical DBs]
    C --> D
```

---

### ðŸ’¾ HDD Volumes â€” High Throughput / Low Cost

#### 3. **st1 â€” Throughput Optimized HDD**

* **Goal:** Low-cost storage for **frequently accessed**, large, sequential data.
* **Use cases:** Big data, data warehousing, log processing, streaming workloads.
* **Performance:**

  * Max Throughput: **500 MB/s**
  * Max IOPS: **500**
  * Volume Size: **Up to 16 TB**

#### 4. **sc1 â€” Cold HDD**

* **Goal:** Lowest-cost HDD for **infrequently accessed** data.
* **Use cases:** Archive, cold data, backups.
* **Performance:**

  * Max Throughput: **250 MB/s**
  * Max IOPS: **250**
  * Volume Size: **Up to 16 TB**

```mermaid
graph TD
    A[HDD Volumes]
    A --> B[st1: Throughput Optimized HDD]
    A --> C[sc1: Cold HDD]
    B --> D[Frequent Access, 500 MB/s, 500 IOPS]
    C --> E[Infrequent Access, 250 MB/s, 250 IOPS]
```

---

### ðŸ”‘ Boot Volume Compatibility

| Volume Type   | Boot Volume Supported? |
| ------------- | ---------------------- |
| **gp2 / gp3** | âœ… Yes                  |
| **io1 / io2** | âœ… Yes                  |
| **st1 / sc1** | âŒ No                   |

---

### ðŸ§  Exam Summary

| Category                 | Volume Type | Performance               | Use Case                    | Notes                                   |
| ------------------------ | ----------- | ------------------------- | --------------------------- | --------------------------------------- |
| **General Purpose SSD**  | gp2/gp3     | Balanced                  | OS boot, general workloads  | gp3 = newer, decoupled IOPS/size        |
| **Provisioned IOPS SSD** | io1/io2     | Highest I/O               | Databases, low latency apps | io2 Block Express = extreme performance |
| **HDD (Throughput)**     | st1         | High throughput, low IOPS | Big data, analytics         | Not bootable                            |
| **HDD (Cold)**           | sc1         | Lowest cost               | Archive, infrequent access  | Not bootable                            |

---

### âš¡ Quick Recall Rules

* **gp3** â†’ Newer, independent IOPS/throughput.
* **gp2** â†’ Older, IOPS tied to size.
* **io1/io2** â†’ Database-grade, provisioned performance.
* **st1/sc1** â†’ HDDs; cheap, throughput-focused, not bootable.
* **>32,000 IOPS â†’ Requires Nitro EC2 + io1/io2.**

---

**Prev**: [EC2 Instance Store](22.EC2InstanceStore.md) | **Next**: [EBS Multi Attach](24.EBSMultiAttach.md) | [Index](../INDEX.md)